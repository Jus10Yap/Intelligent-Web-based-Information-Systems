

How do you load your data from the database to be indexed?

The data is loaded from the MongoDB database using Mongoose. As pages are crawled, they are saved to the database using the Page model. While saving, the pages are also indexed in the elasticlunr index in real-time. Specifically, when a new page is encountered in the crawler's callback, its content is saved to the database and then immediately added to the search index.
What fields do you perform indexing on?

The fields that are indexed are "title" and "body" of the web pages. This means that search queries can match content found within the title or the body of the indexed pages.
How is the search score computed?

The search score is computed by the elasticlunr library. The library uses an inverted index to facilitate fast full-text searches. The score for a document (in this case, a webpage) in response to a query is typically computed based on term frequency-inverse document frequency (TF-IDF) and the vector space model. This means that terms that appear frequently in a document but not in many documents in the corpus will have a higher weight. The exact score also depends on the library's specific algorithms and configurations.
How scalable is your implementation? How could you improve it?

Scalability: The current implementation is moderately scalable. For small to medium-sized datasets, this setup will work quite efficiently. However, as the number of web pages grows substantially, loading every new page into the index during every crawl could become a bottleneck.
Improvements:
Database Indexing: Ensure that database fields queried frequently (like URLs) are indexed in MongoDB to speed up lookups.
Caching: Implement a caching mechanism (like Redis) to store frequently searched terms and their results. This would reduce the time taken for popular queries.
Distributed Crawling: If the number of pages to be crawled is very large, consider distributed crawling solutions that can run across multiple machines.
Batch Indexing: Instead of indexing in real-time during the crawl, pages can be indexed in batches at regular intervals. This might be more efficient for large datasets.
Deduplication: Ensure that only new or changed pages are re-indexed, rather than re-indexing unchanged content. This can be done by checking a hash of the page's content before and after a crawl.
Scaling Search: If search query volume becomes high, consider shifting to a more robust full-text search engine like Elasticsearch.